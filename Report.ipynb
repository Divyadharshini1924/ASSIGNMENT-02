{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Divyadharshini1924/ASSIGNMENT-02/blob/main/Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Convolution Layer\n",
        "\n",
        "import numpy as np\n",
        "from numba import cuda\n",
        "import time\n",
        "\n",
        "#------------Convolution kernel--------------#\n",
        "class Convolution(Layer):\n",
        "    def __init__(self, n_filters=32, filter_size=3, stride=1, activation=None, input_shape=(28, 28, 1)):\n",
        "        self.input_shape = input_shape\n",
        "        self.n_filters = n_filters\n",
        "        self.filter_size = filter_size\n",
        "        self.stride = stride\n",
        "        self.activation = activation\n",
        "        self.use_device = False\n",
        "        self.bias = np.zeros((n_filters, 1))\n",
        "        self.init_weight()\n",
        "\n",
        "    def get_out_shape(self):\n",
        "        output_width = (self.input_shape[2] -\n",
        "                        self.filter_size) // self.stride + 1\n",
        "        output_height = (\n",
        "            self.input_shape[1] - self.filter_size) // self.stride + 1\n",
        "\n",
        "        return ( self.n_filters,output_height, output_width)\n",
        "    def init_weight(self):\n",
        "        self.weights = np.random.randn(\n",
        "            self.n_filters, self.input_shape[0],self.filter_size, self.filter_size)/(self.filter_size**2)\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        n_batchs, n_chanels,in_height, in_width = inputs.shape\n",
        "        assert self.input_shape == inputs.shape[1:], \"Input shape incorrect\"\n",
        "        output_height, output_width = self.get_out_shape()[1:]\n",
        "        ## ===========USING CPU===========##\n",
        "        outputs = np.zeros( (n_batchs, self.n_filters,output_height, output_width ))\n",
        "        for i_idx in range(n_batchs):\n",
        "            for row in range(output_height):\n",
        "                for col in range(output_width):\n",
        "                    for f_idx in range(self.n_filters):\n",
        "                        row_start = row * self.stride\n",
        "                        row_end = row_start + self.filter_size\n",
        "                        col_start = col * self.stride\n",
        "                        col_end = col_start + self.filter_size\n",
        "                        outputs[i_idx,f_idx, row, col] = np.sum(\n",
        "                            self.weights[f_idx] * inputs[i_idx, :, row_start:row_end, col_start:col_end])\n",
        "\n",
        "        if(self.activation == \"relu\"):\n",
        "            outputs = np.maximum(0, outputs)\n",
        "        return outputs\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        n_batchs,input_channels, input_height, input_width = self.inputs.shape\n",
        "        _,n_filters,  output_height, output_width = output_gradient.shape\n",
        "        ## ===========USING CPU===========##\n",
        "        filter_gradient = np.zeros(self.weights.shape)\n",
        "        input_gradient = np.zeros(self.inputs.shape)\n",
        "        for i_batch in range(n_batchs):\n",
        "            for row in range(output_height):\n",
        "                for col in range(output_width):\n",
        "                    for fillterIdx in range(n_filters):\n",
        "                        row_start = row * self.stride\n",
        "                        row_end = row_start + self.filter_size\n",
        "                        col_start = col * self.stride\n",
        "                        col_end = col_start + self.filter_size\n",
        "                        out_grad_val = output_gradient[i_batch,fillterIdx, row, col ]\n",
        "                        filter_gradient[fillterIdx] += self.inputs[i_batch, :,row_start:row_end, col_start:col_end] * out_grad_val\n",
        "                        input_gradient[i_batch, :, row_start:row_end, col_start:col_end] += self.weights[fillterIdx] * out_grad_val\n",
        "        if(self.activation == \"relu\"):\n",
        "            input_gradient[self.inputs <= 0] = 0\n",
        "\n",
        "        self.weights -= learning_rate * filter_gradient/n_batchs\n",
        "\n",
        "        return filter_gradient\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "rwomh7BaU4P0",
        "outputId": "42c731d1-b594-4d05-d8d3-56f5357c9c6c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Layer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-dfb3de10927b>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#------------Convolution kernel--------------#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mConvolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Layer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Eazr70Pw36Gj"
      },
      "outputs": [],
      "source": [
        "# @title Build CNN model\n",
        "import numpy as np\n",
        "from numba import cuda\n",
        "import time\n",
        "# from CNNModel import CNNModel\n",
        "# from layers import Convolution, Flatten, MaxPool2D, Dense\n",
        "class Layer():\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        pass\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        pass\n",
        "\n",
        "    def get_out_shape(self):\n",
        "        pass\n",
        "\n",
        "    def init_weight(self):\n",
        "        pass\n",
        "# from layers_v1 import Layer\n",
        "# import numpy as np\n",
        "\n",
        "\n",
        "class CNNModel:\n",
        "    def __init__(self, layers: list[Layer] = []):\n",
        "        pre_layer = layers[0]\n",
        "        pre_layer.init_weight()\n",
        "        for layer in layers[1:]:\n",
        "            layer.input_shape = pre_layer.get_out_shape()\n",
        "            layer.init_weight()\n",
        "            pre_layer = layer\n",
        "        self.layers: list[Layer] = layers\n",
        "\n",
        "    def forward(self, X):\n",
        "        output = X\n",
        "        for layer in self.layers:\n",
        "            output = layer.forward(output)\n",
        "        return output\n",
        "\n",
        "    def backward(self, out_grad, learning_rate):\n",
        "        for layer in reversed(self.layers):\n",
        "            out_grad = layer.backward(out_grad, learning_rate)\n",
        "        return out_grad\n",
        "\n",
        "    def fit(self, X_train, Y_train, epochs=1, batch_size=32, learning_rate=0.001):\n",
        "        num_batch = (len(X_train)-1)//batch_size+1\n",
        "        for i_epoch in range(epochs):\n",
        "            print(f\"\\nEpoch {i_epoch+1}/{epochs}:\")\n",
        "            train_loss = 0\n",
        "            acc = 0\n",
        "            progress = '.'*30\n",
        "            for i in range(num_batch-1):\n",
        "\n",
        "                batch_start = i * batch_size\n",
        "                batch_end = (i + 1) * batch_size\n",
        "                batch_X = X_train[batch_start: batch_end]\n",
        "                batch_Y = Y_train[batch_start: batch_end]\n",
        "                predictions = self.forward(batch_X)\n",
        "                out_grad = 2.0 * (predictions - batch_Y)\n",
        "                self.backward(out_grad, learning_rate)\n",
        "\n",
        "                # print result\n",
        "                acc_batch = np.mean(\n",
        "                    np.argmax(predictions, axis=1) == np.argmax(batch_Y, axis=1))\n",
        "                acc += acc_batch\n",
        "                loss = np.sum((predictions - batch_Y) ** 2)\n",
        "                train_loss += loss\n",
        "                i_str = int(i/num_batch*30)\n",
        "                progress = progress[:i_str] + \">\" + progress[i_str+1:]\n",
        "                print(\n",
        "                    f\"\\r {i}/{num_batch} [{progress}] accuaray: {acc_batch:.5f}, train loss = {loss/len(batch_Y):.5f}\", end='')\n",
        "                progress = progress[:i_str] + \"=\" + progress[i_str+1:]\n",
        "\n",
        "            train_loss /= len(X_train)\n",
        "\n",
        "            print(\n",
        "                f\"\\r {num_batch}/{num_batch} [{progress}] accuaray: {acc/num_batch:.5f}, train loss = {train_loss:.5f}\", end='')\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "    def use_device(self, value):\n",
        "        for layer in self.layers:\n",
        "            output = layer.use_device = value\n",
        "\n",
        "\n",
        "class Flatten(Layer):\n",
        "    def __init__(self, input_shape=(28, 28, 1)):\n",
        "        self.input_shape = input_shape\n",
        "        pass\n",
        "\n",
        "    def get_out_shape(self):\n",
        "        t = 1\n",
        "        for i in self.input_shape:\n",
        "            t *= i\n",
        "        return t\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        assert self.input_shape == inputs.shape[1:], \"Input shape incorrect\"\n",
        "        return inputs.reshape(inputs.shape[0], -1)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        shape = self.inputs.shape\n",
        "        return output_gradient.reshape(shape)\n",
        "\n",
        "    def init_weight(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14bNBn3mECQW"
      },
      "source": [
        "#II. SEQUENTIAL VERSION V2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSw40TIKECQo"
      },
      "source": [
        "Analysis:\n",
        "\n",
        "- Using loops in python is very slow for calculations with great complexity.\n",
        "- Sequential version 2 improves the use of numpy instead of loops for faster calculations.\n",
        "- The steps to design the Conv, pool, and Dense classes are similar to the sequential version v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ozBh7AyZisDD"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqbkRDXWJ6Gd",
        "outputId": "cf082b75-89f4-4784-a467-b0324bb507d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "'''import pandas as pd\n",
        "import keras\n",
        "\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Assuming you want to load the kidneyData.csv\n",
        "#mnist = \"/content/mnist.csv\"\n",
        "#data = pd.read_csv(mnist)\n",
        "\n",
        "# If you want to split it into train and test sets (assuming train_X, train_y, etc. exist)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # This will prompt you to upload the file\n",
        "\n",
        "\n",
        "# Replace X, y with the actual features and labels in your data\n",
        "X = data.drop(columns=['target'])  # Assuming 'target' is the label column\n",
        "y = data['target']\n",
        "\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Now you have train and test datasets\n",
        "print(train_X.shape, test_X.shape)\n",
        "'''\n",
        "from keras.datasets import mnist\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "y_train = np.zeros((len(train_y),10))\n",
        "y_test = np.zeros((len(test_y),10))\n",
        "for i in range (len(y_train)):\n",
        "  y_train[i,train_y[i]]=1\n",
        "for i in range (len(y_test)):\n",
        "  y_test[i,test_y[i]]=1\n",
        "x_train=train_X.reshape(train_X.shape[0],1, train_X.shape[1], train_X.shape[2])\n",
        "x_test=test_X.reshape(test_X.shape[0],1, test_X.shape[1], test_X.shape[2])\n",
        "x_train=x_train/255\n",
        "x_test=x_test/255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jXRdy2YXECQo"
      },
      "outputs": [],
      "source": [
        "# @title Convolution Layer\n",
        "class Convolution(Layer):\n",
        "    def __init__(self, n_filters=32, filter_size=3, stride=1, activation=None, input_shape=(1, 28, 28)):\n",
        "        self.input_shape = input_shape\n",
        "        self.n_filters = n_filters\n",
        "        self.filter_size = filter_size\n",
        "        self.stride = stride\n",
        "        self.activation = activation\n",
        "        self.use_device = False\n",
        "        self.bias = np.zeros((n_filters, 1))\n",
        "        self.init_weight()\n",
        "\n",
        "    def get_out_shape(self):\n",
        "        output_width = (self.input_shape[2] -\n",
        "                        self.filter_size) // self.stride + 1\n",
        "        output_height = (\n",
        "            self.input_shape[1] - self.filter_size) // self.stride + 1\n",
        "\n",
        "        return ( self.n_filters,output_height, output_width)\n",
        "\n",
        "    def init_weight(self):\n",
        "        np.random.seed(10)\n",
        "        self.weights = np.random.randn(\n",
        "            self.n_filters, self.input_shape[0],self.filter_size, self.filter_size)/(self.filter_size**2)\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        n_batchs, n_chanels,in_height, in_width = inputs.shape\n",
        "        assert self.input_shape == inputs.shape[1:], \"Input shape incorrect\"\n",
        "        output_height, output_width = self.get_out_shape()[1:]\n",
        "        outputs = np.zeros( (n_batchs, self.n_filters,output_height, output_width ))\n",
        "        for row in range(output_height):\n",
        "            for col in range(output_width):\n",
        "                for f_idx in range(self.n_filters):\n",
        "                    row_start = row * self.stride\n",
        "                    row_end = row_start + self.filter_size\n",
        "                    col_start = col * self.stride\n",
        "                    col_end = col_start + self.filter_size\n",
        "                    outputs[:,f_idx, row, col ] = np.sum(\n",
        "                        self.weights[f_idx]*inputs[:, :, row_start:row_end, col_start:col_end],axis=(1,2,3) )\n",
        "\n",
        "        if(self.activation == \"relu\"):\n",
        "            outputs = np.maximum(0, outputs)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        n_batchs,input_channels, input_height, input_width = self.inputs.shape\n",
        "        _,n_filters,  output_height, output_width = output_gradient.shape\n",
        "\n",
        "        filter_gradient = np.zeros(self.weights.shape)\n",
        "        input_gradient = np.zeros(self.inputs.shape)\n",
        "        # for i_batch in range(n_batchs):\n",
        "        for row in range(output_height):\n",
        "            for col in range(output_width):\n",
        "                for fillterIdx in range(n_filters):\n",
        "                    row_start = row * self.stride\n",
        "                    row_end = row_start + self.filter_size\n",
        "                    col_start = col * self.stride\n",
        "                    col_end = col_start + self.filter_size\n",
        "                    out_grad_val = output_gradient[:,fillterIdx, row, col,np.newaxis,np.newaxis,np.newaxis]\n",
        "                    filter_gradient[fillterIdx] +=  np.sum(self.inputs[:, :, row_start:row_end, col_start:col_end] * out_grad_val,axis=0)\n",
        "                    input_gradient[:,: , row_start:row_end, col_start:col_end] += self.weights[fillterIdx] * out_grad_val\n",
        "\n",
        "        if(self.activation == \"relu\"):\n",
        "              input_gradient[self.inputs <= 0] = 0\n",
        "\n",
        "        self.weights -= learning_rate * filter_gradient/n_batchs\n",
        "        return input_gradient\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK-haEBRECQp"
      },
      "source": [
        "* Forward Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMblREnRECQp",
        "outputId": "f165a333-ef19-432b-9ee7-9add037ca244"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 12.9 s, sys: 0 ns, total: 12.9 s\n",
            "Wall time: 13 s\n"
          ]
        }
      ],
      "source": [
        "input_shape=(16,100,100)\n",
        "inputs = np.random.randint(0,255,(32,*input_shape))/255\n",
        "conv = Convolution(32,3,1,input_shape=input_shape)\n",
        "%time out_host=conv.forward(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG33_MBsECQq"
      },
      "source": [
        "* Backward Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrJ9pEjvECQq",
        "outputId": "f60ec2f6-0e3c-4e1c-e96f-d98fcad57312"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 29.2 s, sys: 0 ns, total: 29.2 s\n",
            "Wall time: 29.3 s\n"
          ]
        }
      ],
      "source": [
        "%time in_grad_host=conv.backward(out_host,0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KhdobMK-ECQq"
      },
      "outputs": [],
      "source": [
        "# @title Maxpooling Layer\n",
        "\n",
        "class MaxPool2D(Layer):\n",
        "    def __init__(self, pool_size=2, stride=2, input_shape=(1,28, 28)):\n",
        "        self.pool_size = pool_size\n",
        "        self.stride = stride\n",
        "        self.use_device = False\n",
        "        self.inputs = None\n",
        "        self.inputs_device = None\n",
        "        self.input_shape = input_shape\n",
        "\n",
        "    def get_out_shape(self):\n",
        "        output_height = ( self.input_shape[1] - self.pool_size) // self.stride + 1\n",
        "        output_width = (self.input_shape[2] -  self.pool_size) // self.stride + 1\n",
        "        return (self.input_shape[0],output_height, output_width)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Save input\n",
        "        batch_size,num_channels, input_height, input_width = inputs.shape\n",
        "        assert self.input_shape == inputs.shape[1:], \"Input shape incorrect\"\n",
        "        self.inputs = inputs\n",
        "        ( _,output_height, output_width) = self.get_out_shape()\n",
        "\n",
        "        outputs = np.zeros( (batch_size,num_channels, output_height, output_width))\n",
        "        for h in range(output_height):\n",
        "            for w in range(output_width):\n",
        "                h_start = h * self.stride\n",
        "                h_end = h_start + self.pool_size\n",
        "                w_start = w * self.stride\n",
        "                w_end = w_start + self.pool_size\n",
        "                outputs[:, :,h, w] = np.max( inputs[:, :, h_start:h_end, w_start:w_end], axis=(2, 3))\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        batch_size,num_channels, output_height, output_width = output_gradient.shape\n",
        "        input_gradient = np.zeros(self.inputs.shape)\n",
        "        for h in range(output_height):\n",
        "            for w in range(output_width):\n",
        "                h_start = h * self.stride\n",
        "                h_end = h_start + self.pool_size\n",
        "                w_start = w * self.stride\n",
        "                w_end = w_start + self.pool_size\n",
        "                input_slice = self.inputs[:, :, h_start:h_end, w_start:w_end]\n",
        "                max_vals = np.max(\n",
        "                    input_slice, axis=(2, 3), keepdims=True)\n",
        "                max_mask = (input_slice == max_vals)\n",
        "                input_gradient[:,:, h_start:h_end, w_start:w_end] += max_mask * output_gradient[:,:,  h, w,  np.newaxis, np.newaxis]\n",
        "        return input_gradient\n",
        "    def init_weight(self):\n",
        "        pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjxkj1gzECQr"
      },
      "source": [
        "* Forward Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wb1vxNtzECQr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b164887-45bc-4d35-8d25-b33e00b9f688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.44 s, sys: 91.7 ms, total: 2.53 s\n",
            "Wall time: 2.56 s\n"
          ]
        }
      ],
      "source": [
        "input_shape=(32,200,200)\n",
        "inputs = np.random.randint(0,255,(64,*input_shape))/255\n",
        "maxp = MaxPool2D(2,2,input_shape=input_shape)\n",
        "%time out_host=maxp.forward(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md_jUll8ECQr"
      },
      "source": [
        "* Backward Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7T4IzxbHECQr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bc69a3f-940a-4ecc-a622-8e9bd4f33177"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6.52 s, sys: 638 ms, total: 7.16 s\n",
            "Wall time: 7.24 s\n"
          ]
        }
      ],
      "source": [
        "%time in_grad_host=maxp.backward(out_host,0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8jaVxtnjECQs"
      },
      "outputs": [],
      "source": [
        "# @title Dense Layer\n",
        "class Dense(Layer):\n",
        "    def __init__(self, num_outputs, activation=None, input_shape=100):\n",
        "        self.num_outputs = num_outputs\n",
        "        self.biases = np.zeros((1, num_outputs))\n",
        "        self.activation = activation\n",
        "        self.use_device = False\n",
        "        self.inputs = None\n",
        "        self.input_shape = input_shape\n",
        "        self.init_weight()\n",
        "\n",
        "    def init_weight(self):\n",
        "        self.weights = np.random.randn(\n",
        "            self.input_shape, self.num_outputs) / self.num_outputs\n",
        "\n",
        "    def get_out_shape(self):\n",
        "        return self.num_outputs\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        assert self.input_shape == inputs.shape[-1], \"Input shape incorrect\"\n",
        "        outputs = np.dot(inputs, self.weights) + self.biases\n",
        "        if self.activation == \"softmax\":\n",
        "            outputs = self.softmax(outputs)\n",
        "        return outputs\n",
        "\n",
        "    def softmax(self, x):\n",
        "        e_x = np.exp(x-np.max(x, axis=1, keepdims=True))\n",
        "        return e_x/e_x.sum(axis=1, keepdims=True)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        input_grad = np.dot(output_gradient, self.weights.T)\n",
        "        weights_gradient = np.dot(self.inputs.T, output_gradient)\n",
        "        biases_gradient = np.sum(output_gradient, axis=0, keepdims=True)\n",
        "        self.weights -= learning_rate * weights_gradient\n",
        "        self.biases -= learning_rate * biases_gradient\n",
        "        return input_grad\n",
        "\n",
        "\n",
        "class Flatten(Layer):\n",
        "    def __init__(self, input_shape=(28, 28, 1)):\n",
        "        self.input_shape = input_shape\n",
        "        pass\n",
        "\n",
        "    def get_out_shape(self):\n",
        "        t = 1\n",
        "        for i in self.input_shape:\n",
        "            t *= i\n",
        "        return t\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        assert self.input_shape == inputs.shape[1:], \"Input shape incorrect\"\n",
        "        return inputs.reshape(inputs.shape[0], -1)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        shape = self.inputs.shape\n",
        "        return output_gradient.reshape(shape)\n",
        "\n",
        "    def init_weight(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1PLmeHk7ECQs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6c9dabc-6dec-4f6b-a833-4db4f06ec5ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 304 ms, sys: 10.8 ms, total: 315 ms\n",
            "Wall time: 166 ms\n"
          ]
        }
      ],
      "source": [
        "inputs=np.random.randint(1,255, (256,10000))/255\n",
        "dense=Dense(1024, input_shape= 10000)\n",
        "%time out_host=dense.forward(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC2IJg3iKtLd"
      },
      "source": [
        "## Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "qiiItW8MKhE2"
      },
      "outputs": [],
      "source": [
        "modelII=CNNModel([\n",
        "    Convolution(n_filters=16, filter_size=3, stride=1,activation='relu',input_shape=(1,28,28)),\n",
        "    MaxPool2D(pool_size=2),\n",
        "    Convolution(n_filters=32, filter_size=3, stride=1,activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(128),\n",
        "    Dense(10, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2GUEMtoKzM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cf7b5fb-2457-412d-e06b-549eee9da530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3:\n",
            " 30/469 [=>............................] accuaray: 0.85156, train loss = 0.26191"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "modelII.fit(x_train,y_train, epochs=3, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSPkbgIyVqml"
      },
      "outputs": [],
      "source": [
        "%time y_predict =modelII.predict(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OurWW4rgGXo5"
      },
      "source": [
        "#III. PARALLEL VERSION V1\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTOtAqlFGXpO"
      },
      "source": [
        "Analysis:\n",
        "\n",
        "-The first parallel version simply maps the sequential version using grid and thread instead of loops\n",
        "Have not used optimization techniques such as streaming, using SMEM, reducing wap divergence, choosing block size,...\n",
        "\n",
        "-There is a connection between the layers (the output of one layer is the input of the next layer), so we cannot parallelize the calculations of the layers at the same time, but must separate the tasks to parallelize them.\n",
        "\n",
        "-The time-consuming computational work mainly falls into calculating forward and backward, so we just need to parallelize these functions on each layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqfppCsjGXpO"
      },
      "outputs": [],
      "source": [
        "# @title Convolution Layer\n",
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "#------------Convolution kernel--------------#\n",
        "@cuda.jit\n",
        "def conv_forward_kernel(inputs, weights, stride, outputs, activation):\n",
        "    n_chanels=inputs.shape[1]\n",
        "    filter_size= weights.shape[-1]\n",
        "    n_batch, n_filters,output_height, output_width = outputs.shape\n",
        "    i_batch, row, col = cuda.grid(3)\n",
        "    if(row >= output_height or col >= output_width or i_batch >= n_batch):\n",
        "        return\n",
        "\n",
        "    for fillterIdx in range(n_filters):\n",
        "        sum = 0\n",
        "        for chanel_idx in range(n_chanels):\n",
        "            for fillterRow in range(filter_size):\n",
        "                for fillterCol in range(filter_size):\n",
        "                    iR = row*stride + fillterRow\n",
        "                    iC = col*stride + fillterCol\n",
        "                    sum += inputs[i_batch,chanel_idx, iR, iC] * weights[fillterIdx,chanel_idx, fillterRow, fillterCol]\n",
        "        if(activation == 1 and sum < 0):\n",
        "            sum = 0\n",
        "        outputs[i_batch,fillterIdx, row, col] = sum\n",
        "\n",
        "\n",
        "@cuda.jit\n",
        "def conv_backward_kernel(input, weights, stride, input_gradient, output_gradient, filter_gradient, activation):\n",
        "    n_chanels,filter_size = weights.shape[1:-1]\n",
        "    n_batch,n_filters, output_height, output_width  = output_gradient.shape\n",
        "    i_batch, row, col = cuda.grid(3)\n",
        "    if(row >= output_height or col >= output_width or i_batch >= n_batch):\n",
        "        return\n",
        "\n",
        "    for fillterIdx in range(n_filters):\n",
        "        for fillterRow in range(filter_size):\n",
        "            for fillterCol in range(filter_size):\n",
        "                out_value = output_gradient[i_batch, fillterIdx,row, col]\n",
        "                for i_chanel in range(n_chanels):\n",
        "                    iR = row*stride + fillterRow\n",
        "                    iC = col*stride + fillterCol\n",
        "                    in_val = input[i_batch, i_chanel,iR, iC]\n",
        "                    cuda.atomic.add(\n",
        "                        filter_gradient, (fillterIdx, i_chanel,fillterRow, fillterCol), input[i_batch, i_chanel,iR, iC] * out_value)\n",
        "                    if(not (in_val <= 0 and activation == 1)):\n",
        "                      cuda.atomic.add(input_gradient, (i_batch, i_chanel,iR, iC),\n",
        "                                          weights[fillterIdx,i_chanel, fillterRow, fillterCol] * out_value)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Convolution(Layer):\n",
        "    def __init__(self, n_filters=32, filter_size=3, stride=1, activation=None, input_shape=(28, 28, 1)):\n",
        "        self.input_shape = input_shape\n",
        "        self.n_filters = n_filters\n",
        "        self.filter_size = filter_size\n",
        "        self.stride = stride\n",
        "        self.activation = activation\n",
        "        self.use_device = False\n",
        "        self.bias = np.zeros((n_filters, 1))\n",
        "        self.init_weight()\n",
        "\n",
        "    def get_out_shape(self):\n",
        "        output_width = (self.input_shape[2] -\n",
        "                        self.filter_size) // self.stride + 1\n",
        "        output_height = (\n",
        "            self.input_shape[1] - self.filter_size) // self.stride + 1\n",
        "\n",
        "        return ( self.n_filters,output_height, output_width)\n",
        "\n",
        "\n",
        "    def init_weight(self):\n",
        "        self.weights = np.random.randn(\n",
        "            self.n_filters, self.input_shape[0],self.filter_size, self.filter_size)/(self.filter_size**2)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        self.inputs = inputs\n",
        "        n_batchs, n_chanels,in_height, in_width = inputs.shape\n",
        "        assert self.input_shape == inputs.shape[1:], \"Input shape incorrect\"\n",
        "        output_height, output_width = self.get_out_shape()[1:]\n",
        "        block_size = (8, 8, 8)\n",
        "        grid_size = ((n_batchs-1)//block_size[0]+1, (output_height-1) //\n",
        "                      block_size[1]+1, (output_width-1)//block_size[2]+1)\n",
        "        d_outputs = cuda.device_array((n_batchs, self.n_filters,output_height, output_width ))\n",
        "        self.d_weights = cuda.to_device(self.weights)\n",
        "        self.d_inputs = cuda.to_device(self.inputs)\n",
        "        conv_forward_kernel[grid_size, block_size](\n",
        "            self.d_inputs, self.d_weights, 1, d_outputs, int(self.activation == \"relu\"))\n",
        "        outputs = d_outputs.copy_to_host()\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        n_batchs,input_channels, input_height, input_width = self.inputs.shape\n",
        "        _,n_filters,  output_height, output_width = output_gradient.shape\n",
        "        block_size = (4, 4, 4)\n",
        "        grid_size = ((n_batchs-1)//block_size[0]+1, (output_height-1) //\n",
        "                      block_size[1]+1, (output_width-1)//block_size[2]+1)\n",
        "        d_filter_grad = cuda.device_array(self.weights.shape)\n",
        "        d_input_grad = cuda.device_array(self.inputs.shape)\n",
        "        d_output_grad = cuda.to_device(output_gradient)\n",
        "        # call kernel\n",
        "        conv_backward_kernel[grid_size, block_size](\n",
        "            self.d_inputs, self.d_weights, 1, d_input_grad, d_output_grad, d_filter_grad, int(self.activation == \"relu\"))\n",
        "        cuda.synchronize()\n",
        "        input_gradient = d_input_grad.copy_to_host()\n",
        "        filter_gradient = d_filter_grad.copy_to_host()\n",
        "        ## ===========END USING DEVICE===========##\n",
        "        self.weights -= learning_rate * filter_gradient/n_batchs\n",
        "\n",
        "        return input_gradient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4uUPWg1mG8g"
      },
      "source": [
        "Design - Convolution\n",
        "\n",
        "**+Forward:**\n",
        "**Input:** Input, weights, stride, activation\n",
        "-How to install kernel function:\n",
        "-The output dimension will be ( n_batch, out_height, out_width, n_filter)\n",
        "-Each thread will be responsible for 1 output element including n_batch, out_height, out_width.\n",
        "-Using 3d block and 3d grid, choose x dimension corresponding to n_batch, y corresponding to out_height, z corresponding to out_width\n",
        "**Output:** Result matrix “output”\n",
        "\n",
        "**+Backward:**\n",
        "**Input: **Input,output_grad, weight\n",
        "How to install kernel function:\n",
        "-The dimension of output_grad is (n_batch, out_height, out_width, n_filter))\n",
        "-Each thread will be responsible for 1 element in output_grad including n_batch, out_height, out_width.\n",
        "-Using 3d block and 3d grid, choose x dimension corresponding to n_batch, y corresponding to out_height, z corresponding to out_width\n",
        "-In addition, the values ​​of Input_grad and weight_grad will accumulate between threads at the same time, so you need to use the atomic_add function to accumulate the value of the element.\n",
        "\n",
        "**Output:**Input_grad matrix, weight_grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TwX1UPqGXpP"
      },
      "source": [
        "* Forward Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh6lBE1-GXpP"
      },
      "outputs": [],
      "source": [
        "input_shape=(16,100,100)\n",
        "inputs = np.random.randint(0,255,(32,*input_shape))/255\n",
        "conv = Convolution(32,3,1,input_shape=input_shape)\n",
        "%time out_host=conv.forward(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5RVz5xnGXpQ"
      },
      "source": [
        "* Backward Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqTVa4T0GXpQ"
      },
      "outputs": [],
      "source": [
        "%time in_grad_host=conv.backward(out_host,0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6iPsh1_GXpR"
      },
      "outputs": [],
      "source": [
        "# @title Maxpooling Layer\n",
        "#------------MaxPool2D kernel--------------#\n",
        "@cuda.jit\n",
        "def maxPool2D_forward_kernel(inputs, outputs, stride, pool_size):\n",
        "    n_batchs,n_chanels, in_height, in_width = inputs.shape\n",
        "    n_batchs,n_chanels, output_height, output_width = outputs.shape\n",
        "    ibatch, out_h, out_w = cuda.grid(3)\n",
        "    # Max pool over input\n",
        "    if(ibatch >= n_batchs or out_h >= output_height or out_w >= output_width):\n",
        "        return\n",
        "\n",
        "    for i_chanel in range(n_chanels):\n",
        "        max_value = -np.inf\n",
        "        for h_pool in range(pool_size):\n",
        "            for w_pool in range(pool_size):\n",
        "                max_value = max(\n",
        "                    max_value, inputs[ibatch, i_chanel,out_h*stride+h_pool, w_pool+out_w*stride])\n",
        "        outputs[ibatch,i_chanel, out_h, out_w] = max_value\n",
        "\n",
        "\n",
        "@cuda.jit\n",
        "def maxPool2D_backward_kernel(inputs, inputs_grad, outputs_grad, stride, pool_size):\n",
        "    n_batchs,n_chanels, in_height, in_width = inputs.shape\n",
        "    n_batchs,n_chanels, output_height, output_width,  = outputs_grad.shape\n",
        "    ibatch, out_h, out_w = cuda.grid(3)\n",
        "    # Max pool over input\n",
        "    if(ibatch >= n_batchs or out_h >= output_height or out_w >= output_width):\n",
        "        return\n",
        "    for i_chanel in range(n_chanels):\n",
        "        max_value = -np.inf\n",
        "        for h_pool in range(pool_size):\n",
        "            for w_pool in range(pool_size):\n",
        "                max_value = max(\n",
        "                    max_value, inputs[ibatch, i_chanel,out_h*stride+h_pool, w_pool+out_w*stride])\n",
        "\n",
        "        for h_pool in range(pool_size):\n",
        "            for w_pool in range(pool_size):\n",
        "                if(inputs[ibatch,i_chanel ,out_h*stride+h_pool, w_pool+out_w*stride] == max_value):\n",
        "                    inputs_grad[ibatch, i_chanel,out_h*stride+h_pool, w_pool+out_w*stride] += outputs_grad[ibatch,i_chanel,  out_h, out_w]\n",
        "\n",
        "\n",
        "class MaxPool2D(Layer):\n",
        "    def __init__(self, pool_size=2, stride=2, input_shape=(28, 28, 1)):\n",
        "        self.pool_size = pool_size\n",
        "        self.stride = stride\n",
        "        self.use_device = False\n",
        "        self.inputs = None\n",
        "        self.inputs_device = None\n",
        "        self.input_shape = input_shape\n",
        "\n",
        "    def get_out_shape(self):\n",
        "        output_height = ( self.input_shape[1] - self.pool_size) // self.stride + 1\n",
        "        output_width = (self.input_shape[2] -  self.pool_size) // self.stride + 1\n",
        "        return (self.input_shape[0],output_height, output_width)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Save input\n",
        "        batch_size,num_channels, input_height, input_width = inputs.shape\n",
        "        assert self.input_shape == inputs.shape[1:], \"Input shape incorrect\"\n",
        "        self.inputs = inputs\n",
        "        ( _,output_height, output_width) = self.get_out_shape()\n",
        "        d_outputs = cuda.device_array(\n",
        "            (batch_size,num_channels, output_height, output_width))\n",
        "        block_size = (8, 4, 4)\n",
        "        grid_size = ((batch_size-1)//block_size[0]+1, (output_height-1) //\n",
        "                      block_size[1]+1, (output_width-1)//block_size[2]+1)\n",
        "        self.d_inputs = cuda.to_device(inputs)\n",
        "        maxPool2D_forward_kernel[grid_size, block_size](\n",
        "            self.d_inputs, d_outputs, self.stride, self.pool_size)\n",
        "        outputs = d_outputs.copy_to_host()\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        batch_size,num_channels, output_height, output_width = output_gradient.shape\n",
        "        d_input_grad = cuda.device_array(self.inputs.shape)\n",
        "        d_output_grad = cuda.to_device(output_gradient)\n",
        "        block_size = (8, 4, 4)\n",
        "        grid_size = ((batch_size-1)//block_size[0]+1, (output_height-1) //\n",
        "                      block_size[1]+1, (output_width-1)//block_size[2]+1)\n",
        "        maxPool2D_backward_kernel[grid_size, block_size](\n",
        "            self.d_inputs, d_input_grad, d_output_grad, self.stride, self.pool_size)\n",
        "        input_gradient = d_input_grad.copy_to_host()\n",
        "        return input_gradient\n",
        "\n",
        "\n",
        "    def init_weight(self):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSoc5oVGmX3j"
      },
      "source": [
        "Design - Convolution\n",
        "\n",
        "**+Forward:**\n",
        "**Input:** Input, stride, pool size\n",
        "-How to install kernel function:\n",
        "-The output dimension will be ( n_batch, out_height, out_width, n_channel)\n",
        "-Each thread will be responsible for 1 output element including n_batch, out_height, out_width.\n",
        "-Using 3d block and 3d grid, choose x dimension corresponding to n_batch, y corresponding to out_height, z corresponding to out_width\n",
        "Output: Ma trận kết quả “output”\n",
        "\n",
        "**+Backward:**\n",
        "Input: Input,output_grad, stride, pool_size\n",
        "-How to install kernel function:\n",
        "-The output_grad dimension is (n_batch, out_height, out_width, n_channel)\n",
        "-Each thread will be responsible for 1 element in output_grad including n_batch, out_height, out_width.\n",
        "-Using 3d block and 3d grid, choose x dimension corresponding to n_batch, y corresponding to out_height, z corresponding to out_width\n",
        "**Output:** Input_grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD_SLG9sGXpR"
      },
      "source": [
        "* Forward Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gm9ym3bNGXpR"
      },
      "outputs": [],
      "source": [
        "input_shape=(32,200,200)\n",
        "inputs = np.random.randint(0,255,(64,*input_shape))/255\n",
        "maxp = MaxPool2D(2,2,input_shape=input_shape)\n",
        "%time out_host=maxp.forward(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jymcBSgUGXpS"
      },
      "source": [
        "* Backward Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0I-PS8OGXpS"
      },
      "outputs": [],
      "source": [
        "%time in_grad_host=maxp.backward(out_host,0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUEuPcWsGXpS"
      },
      "outputs": [],
      "source": [
        "# @title Dense Layer\n",
        "\n",
        "@cuda.jit\n",
        "def dense_forward_kernel(inputs, weights, bias, outputs):\n",
        "    row, col = cuda.grid(2)\n",
        "    height = weights.shape[0]\n",
        "    if(row >= outputs.shape[0] or col >= outputs.shape[1]):\n",
        "        return\n",
        "    sum = 0\n",
        "    for i in range(inputs.shape[1]):\n",
        "        sum += inputs[row, i] * weights[i, col]\n",
        "    outputs[row, col] = sum + bias[0, col]\n",
        "\n",
        "\n",
        "\n",
        "class Dense(Layer):\n",
        "    def __init__(self, num_outputs, activation=None, input_shape=100):\n",
        "        self.num_outputs = num_outputs\n",
        "        self.biases = np.zeros((1, num_outputs))\n",
        "        self.activation = activation\n",
        "        self.use_device = False\n",
        "        self.inputs = None\n",
        "        self.input_shape = input_shape\n",
        "        self.init_weight()\n",
        "\n",
        "    def init_weight(self):\n",
        "        self.weights = np.random.randn(\n",
        "            self.input_shape, self.num_outputs) / self.num_outputs\n",
        "\n",
        "    def get_out_shape(self):\n",
        "        return self.num_outputs\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        assert self.input_shape == inputs.shape[-1], \"Input shape incorrect\"\n",
        "        block_size = (8, 4)\n",
        "        grid_size = ((inputs.shape[0]-1)//block_size[0]+1,\n",
        "                     (self.num_outputs-1)//block_size[1]+1)\n",
        "        # if(grid_size[0]*grid_size[1] < 128):\n",
        "        #     self.use_device = False\n",
        "        # outputs = None\n",
        "        # if(self.use_device == False):\n",
        "        #     outputs = np.dot(inputs, self.weights) + self.biases\n",
        "        # else:\n",
        "        self.d_weights = cuda.to_device(self.weights)\n",
        "        self.d_biases = cuda.to_device(self.biases)\n",
        "        d_outputs = cuda.device_array((inputs.shape[0], self.num_outputs))\n",
        "        self.d_inputs = cuda.to_device(inputs)\n",
        "        start = time.time()\n",
        "        dense_forward_kernel[grid_size, block_size](\n",
        "            self.d_inputs, self.d_weights, self.d_biases, d_outputs)\n",
        "        outputs = d_outputs.copy_to_host()\n",
        "\n",
        "        # if(self.activation==\"relu\"):\n",
        "        #   outputs = np.maximum(0,outputs)\n",
        "        if self.activation == \"softmax\":\n",
        "            outputs = self.softmax(outputs)\n",
        "        return outputs\n",
        "\n",
        "    def softmax(self, x):\n",
        "        e_x = np.exp(x-np.max(x, axis=1, keepdims=True))\n",
        "        return e_x/e_x.sum(axis=1, keepdims=True)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        # start = time.time()\n",
        "        # input_grad=None\n",
        "        # if(self.use_device==False):\n",
        "\n",
        "        input_grad = np.dot(output_gradient, self.weights.T)\n",
        "        weights_gradient = np.dot(self.inputs.T, output_gradient)\n",
        "        biases_gradient = np.sum(output_gradient, axis=0, keepdims=True)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights -= learning_rate * weights_gradient\n",
        "        self.biases -= learning_rate * biases_gradient\n",
        "\n",
        "        return input_grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWMfwaIPmn_U"
      },
      "source": [
        "Design- Convolution\n",
        "\n",
        "**+Forward:**\n",
        "**Input:** Input, weigth, bias\n",
        "-How to install kernel function:\n",
        "  The output direction will be ( n_batch, n_out)\n",
        "  Each thread will be responsible for 1 output element including n_batch, n_out,\n",
        "  Using 2d block and 2d grid, choose x dimension corresponding to n_batch, y corresponding to n_out,\n",
        "**Output:** Result matrix “output”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "earYPbQUGXpT"
      },
      "outputs": [],
      "source": [
        "inputs=np.random.randint(1,255, (256,10000))/255\n",
        "dense=Dense(1024, input_shape= 10000)\n",
        "%time out_host=dense.forward(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFGH8FAJNXL3"
      },
      "source": [
        "## Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4j40AY4NJ7V"
      },
      "outputs": [],
      "source": [
        "modelIII=CNNModel([\n",
        "    Convolution(n_filters=16, filter_size=3, stride=1,activation='relu',input_shape=(1,28,28)),\n",
        "    MaxPool2D(pool_size=2),\n",
        "    Convolution(n_filters=32, filter_size=3, stride=1,activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(128),\n",
        "    Dense(10, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJq007_aNnlP"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "modelIII.fit(x_train,y_train, epochs=3, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUcDUw3yvQtq"
      },
      "outputs": [],
      "source": [
        "%time y_predict =modelIII.predict(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCOTy97lN6HZ"
      },
      "source": [
        "# IV. PARALLEL VERSION V2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1ycG-TPN6Hf"
      },
      "source": [
        "Analysis:\n",
        "\n",
        "- The first parallel version simply maps the sequential version using grid and thread instead of loops\n",
        "- However, in this improved version, we will optimize memory access, limit the use of GMEM by using RMEM + SMEM to help access memory.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUa3Kgo4N6Hg"
      },
      "outputs": [],
      "source": [
        "# @title Convolution Layer\n",
        "import numpy as np\n",
        "from numba import cuda, types as numba_types\n",
        "\n",
        "#------------Convolution kernel--------------#\n",
        "@cuda.jit\n",
        "def conv_forward_kernel(d_inputs, weights, stride, outputs, activation):\n",
        "    shared_input = cuda.shared.array((8,6,6),numba_types.float32)\n",
        "    i_batch, row, col = cuda.grid(3)\n",
        "    n_batch, n_filters,output_height, output_width = outputs.shape\n",
        "    if( i_batch >= n_batch): return\n",
        "    filter_size= weights.shape[-1]\n",
        "    n_chanels=d_inputs.shape[1]\n",
        "    tx = cuda.threadIdx.x\n",
        "    ty = cuda.threadIdx.y\n",
        "    tz = cuda.threadIdx.z\n",
        "    by=cuda.blockIdx.y*cuda.blockDim.y\n",
        "    bz=cuda.blockIdx.z*cuda.blockDim.z\n",
        "    for fillterIdx in range(n_filters):\n",
        "        sum_val = 0\n",
        "        for chanel_idx in range(n_chanels):\n",
        "            for i in range(4):\n",
        "              idx=(ty*4+tz)+i*16\n",
        "              y,z=idx//6,idx%6\n",
        "              if(y>=6): break\n",
        "              shared_input[tx,y,z] =d_inputs[i_batch,chanel_idx,by+y, bz+z]\n",
        "            cuda.syncthreads()\n",
        "            for fillterRow in range(filter_size):\n",
        "                for fillterCol in range(filter_size):\n",
        "                    sum_val +=shared_input[tx,fillterRow+ty,fillterCol+tz]*weights[fillterIdx,chanel_idx, fillterRow, fillterCol]\n",
        "            cuda.syncthreads()\n",
        "\n",
        "        if not (row >= output_height or col >= output_width):\n",
        "          if(activation == 1 and sum_val < 0):\n",
        "              sum_val = 0\n",
        "          outputs[i_batch,fillterIdx, row, col] = sum_val\n",
        "\n",
        "\n",
        "@cuda.jit\n",
        "def conv_backward_kernel(d_inputs, weights, stride, input_gradient, output_gradient, filter_gradient, activation):\n",
        "    shared_input = cuda.shared.array((8,6,6),numba_types.float32)\n",
        "    n_chanels,filter_size = weights.shape[1:-1]\n",
        "    n_batch,n_filters, output_height, output_width  = output_gradient.shape\n",
        "    i_batch, row, col = cuda.grid(3)\n",
        "    # if(row >= output_height or col >= output_width or i_batch >= n_batch):\n",
        "    #     return\n",
        "    if(i_batch >= n_batch):\n",
        "      return\n",
        "    tx = cuda.threadIdx.x\n",
        "    ty = cuda.threadIdx.y\n",
        "    tz = cuda.threadIdx.z\n",
        "    by=cuda.blockIdx.y*cuda.blockDim.y\n",
        "    bz=cuda.blockIdx.z*cuda.blockDim.z\n",
        "    for fillterIdx in range(n_filters):\n",
        "        out_value = output_gradient[i_batch, fillterIdx,row, col]\n",
        "        for i_chanel in range(n_chanels):\n",
        "          for i in range(3):\n",
        "              idx=(ty*4+tz)+i*16\n",
        "              y,z=idx//6,idx%6\n",
        "              if(y>=6): break\n",
        "              shared_input[tx,y,z] =d_inputs[i_batch,i_chanel,by+y, bz+z]\n",
        "          cuda.syncthreads()\n",
        "          if not (row >= output_height or col >= output_width):\n",
        "            for fillterRow in range(filter_size):\n",
        "                for fillterCol in range(filter_size):\n",
        "                        iR = row*stride + fillterRow\n",
        "                        iC = col*stride + fillterCol\n",
        "                        in_val = d_inputs[i_batch, i_chanel,iR, iC]\n",
        "                        cuda.atomic.add(\n",
        "                            filter_gradient, (fillterIdx, i_chanel,fillterRow, fillterCol), in_val* out_value)\n",
        "                        if(not (in_val <= 0 and activation == 1)):\n",
        "                          cuda.atomic.add(input_gradient, (i_batch, i_chanel,iR, iC),\n",
        "                                              weights[fillterIdx,i_chanel, fillterRow, fillterCol] * out_value)\n",
        "          cuda.syncthreads()\n",
        "\n",
        "\n",
        "class Convolution(Layer):\n",
        "    def __init__(self, n_filters=32, filter_size=3, stride=1, activation=None, input_shape=(28, 28, 1)):\n",
        "        self.input_shape = input_shape\n",
        "        self.n_filters = n_filters\n",
        "        self.filter_size = filter_size\n",
        "        self.stride = stride\n",
        "        self.activation = activation\n",
        "        self.use_device = False\n",
        "        self.bias = np.zeros((n_filters, 1))\n",
        "        self.init_weight()\n",
        "\n",
        "    def get_out_shape(self):\n",
        "        output_width = (self.input_shape[2] -\n",
        "                        self.filter_size) // self.stride + 1\n",
        "        output_height = (\n",
        "            self.input_shape[1] - self.filter_size) // self.stride + 1\n",
        "\n",
        "        return ( self.n_filters,output_height, output_width)\n",
        "\n",
        "\n",
        "    def init_weight(self):\n",
        "        self.weights = np.random.randn(\n",
        "            self.n_filters, self.input_shape[0],self.filter_size, self.filter_size)/(self.filter_size**2)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        self.inputs = inputs\n",
        "        n_batchs, n_chanels,in_height, in_width = inputs.shape\n",
        "        assert self.input_shape == inputs.shape[1:], \"Input shape incorrect\"\n",
        "        output_height, output_width = self.get_out_shape()[1:]\n",
        "        block_size = (8, 4, 4)\n",
        "        grid_size = ((n_batchs-1)//block_size[0]+1, (output_height-1) //\n",
        "                      block_size[1]+1, (output_width-1)//block_size[2]+1)\n",
        "        d_outputs = cuda.device_array((n_batchs, self.n_filters,output_height, output_width ))\n",
        "        self.d_weights = cuda.to_device(self.weights)\n",
        "        self.d_inputs = cuda.to_device(self.inputs)\n",
        "        conv_forward_kernel[grid_size, block_size](\n",
        "            self.d_inputs, self.d_weights, 1, d_outputs, int(self.activation == \"relu\"))\n",
        "        outputs = d_outputs.copy_to_host()\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        n_batchs,input_channels, input_height, input_width = self.inputs.shape\n",
        "        _,n_filters,  output_height, output_width = output_gradient.shape\n",
        "        block_size = (8, 4, 4)\n",
        "        grid_size = ((n_batchs-1)//block_size[0]+1, (output_height-1) //\n",
        "                      block_size[1]+1, (output_width-1)//block_size[2]+1)\n",
        "        d_filter_grad = cuda.device_array(self.weights.shape)\n",
        "        d_input_grad = cuda.device_array(self.inputs.shape)\n",
        "        d_output_grad = cuda.to_device(output_gradient)\n",
        "        # call kernel\n",
        "        conv_backward_kernel[grid_size, block_size](\n",
        "            self.d_inputs, self.d_weights, 1, d_input_grad, d_output_grad, d_filter_grad, int(self.activation == \"relu\"))\n",
        "        cuda.synchronize()\n",
        "        input_gradient = d_input_grad.copy_to_host()\n",
        "        filter_gradient = d_filter_grad.copy_to_host()\n",
        "        ## ===========END USING DEVICE===========##\n",
        "        self.weights -= learning_rate * filter_gradient/n_batchs\n",
        "\n",
        "        return input_gradient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmVi4_gnok98"
      },
      "source": [
        "Design - Convolution\n",
        "\n",
        "The design steps are similar to parallel version v1\n",
        "However, here we will use SMEM to save input in the same block for reuse many times\n",
        "In addition, in the backward kernel version we use RMEM to store output_grad, helping to limit multiple accesses to GMEM.\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSGvUEPhN6Hg"
      },
      "source": [
        "* Forward Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIeTXovdN6Hg"
      },
      "outputs": [],
      "source": [
        "input_shape=(16,100,100)\n",
        "inputs = np.random.randint(0,255,(32,*input_shape))/255\n",
        "conv = Convolution(32,3,1,input_shape=input_shape)\n",
        "%time out_host=conv.forward(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI7Tz5dNN6Hg"
      },
      "source": [
        "* Backward Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3IkMlM-N6Hh"
      },
      "outputs": [],
      "source": [
        "%time in_grad_host=conv.backward(out_host,0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIvKSzNMN6Hh"
      },
      "outputs": [],
      "source": [
        "# @title Maxpooling Layer\n",
        "@cuda.jit\n",
        "def maxPool2D_forward_kernel(d_inputs, outputs, stride, pool_size):\n",
        "    share_size = 8\n",
        "    shared_input = cuda.shared.array((8,share_size,share_size),numba_types.float32)\n",
        "    n_batchs,n_chanels, in_height, in_width = d_inputs.shape\n",
        "    n_batchs,n_chanels, output_height, output_width = outputs.shape\n",
        "    i_batch, out_h, out_w = cuda.grid(3)\n",
        "    # Max pool over input\n",
        "    if(i_batch >= n_batchs): return\n",
        "    tx = cuda.threadIdx.x\n",
        "    ty = cuda.threadIdx.y\n",
        "    tz = cuda.threadIdx.z\n",
        "    by=cuda.blockIdx.y*cuda.blockDim.y * stride\n",
        "    bz=cuda.blockIdx.z*cuda.blockDim.z * stride\n",
        "\n",
        "    for i_chanel in range(n_chanels):\n",
        "        for i in range(4):\n",
        "          idx=(ty*4+tz)+i*16\n",
        "          y,z=idx//share_size,idx%share_size\n",
        "          if(y>=share_size): break\n",
        "          shared_input[tx,y,z] =d_inputs[i_batch,i_chanel,by+y, bz+z]\n",
        "        cuda.syncthreads()\n",
        "        max_value = -np.inf\n",
        "        if not (out_h >= output_height or out_w >= output_width):\n",
        "          for h_pool in range(pool_size):\n",
        "              for w_pool in range(pool_size):\n",
        "                  max_value = max(\n",
        "                      # max_value, d_inputs[i_batch, i_chanel,out_h*stride+h_pool, w_pool+out_w*stride])\n",
        "                      max_value, shared_input[tx,ty*stride+h_pool, w_pool+tz*stride])\n",
        "          outputs[i_batch,i_chanel, out_h, out_w] = max_value\n",
        "        cuda.syncthreads()\n",
        "\n",
        "@cuda.jit\n",
        "def maxPool2D_backward_kernel(d_inputs, inputs_grad, outputs_grad, stride, pool_size):\n",
        "    share_size = 8\n",
        "    shared_input = cuda.shared.array((8,share_size,share_size),numba_types.float32)\n",
        "    n_batchs,n_chanels, in_height, in_width = d_inputs.shape\n",
        "    n_batchs,n_chanels, output_height, output_width,  = outputs_grad.shape\n",
        "    i_batch, out_h, out_w = cuda.grid(3)\n",
        "    # Max pool over input\n",
        "    if(i_batch >= n_batchs ):\n",
        "        return\n",
        "    tx = cuda.threadIdx.x\n",
        "    ty = cuda.threadIdx.y\n",
        "    tz = cuda.threadIdx.z\n",
        "    by=cuda.blockIdx.y*cuda.blockDim.y * stride\n",
        "    bz=cuda.blockIdx.z*cuda.blockDim.z * stride\n",
        "    for i_chanel in range(n_chanels):\n",
        "        for i in range(4):\n",
        "          idx=(ty*4+tz)+i*16\n",
        "          y,z=idx//share_size,idx%share_size\n",
        "          if(y>=share_size): break\n",
        "          shared_input[tx,y,z] =d_inputs[i_batch,i_chanel,by+y, bz+z]\n",
        "        cuda.syncthreads()\n",
        "\n",
        "        max_value = -np.inf\n",
        "        if not (out_h >= output_height or out_w >= output_width):\n",
        "          for h_pool in range(pool_size):\n",
        "              for w_pool in range(pool_size):\n",
        "                  max_value = max( max_value,  shared_input[tx,ty*stride+h_pool, w_pool+tz*stride])\n",
        "\n",
        "          for h_pool in range(pool_size):\n",
        "              for w_pool in range(pool_size):\n",
        "                  if( shared_input[tx,ty*stride+h_pool, w_pool+tz*stride] ==max_value):\n",
        "                      inputs_grad[i_batch, i_chanel,out_h*stride+h_pool, w_pool+out_w*stride] += outputs_grad[i_batch,i_chanel,  out_h, out_w]\n",
        "        cuda.syncthreads()\n",
        "#------------Linear kernel--------------#\n",
        "\n",
        "class MaxPool2D(Layer):\n",
        "    def __init__(self, pool_size=2, stride=2, input_shape=(28, 28, 1)):\n",
        "        self.pool_size = pool_size\n",
        "        self.stride = stride\n",
        "        self.use_device = False\n",
        "        self.inputs = None\n",
        "        self.inputs_device = None\n",
        "        self.input_shape = input_shape\n",
        "\n",
        "    def get_out_shape(self):\n",
        "        output_height = ( self.input_shape[1] - self.pool_size) // self.stride + 1\n",
        "        output_width = (self.input_shape[2] -  self.pool_size) // self.stride + 1\n",
        "        return (self.input_shape[0],output_height, output_width)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Save input\n",
        "        batch_size,num_channels, input_height, input_width = inputs.shape\n",
        "        assert self.input_shape == inputs.shape[1:], \"Input shape incorrect\"\n",
        "        self.inputs = inputs\n",
        "        ( _,output_height, output_width) = self.get_out_shape()\n",
        "        d_outputs = cuda.device_array(\n",
        "            (batch_size,num_channels, output_height, output_width))\n",
        "        block_size = (8, 4, 4)\n",
        "        grid_size = ((batch_size-1)//block_size[0]+1, (output_height-1) //\n",
        "                      block_size[1]+1, (output_width-1)//block_size[2]+1)\n",
        "        self.d_inputs = cuda.to_device(inputs)\n",
        "        maxPool2D_forward_kernel[grid_size, block_size](\n",
        "            self.d_inputs, d_outputs, self.stride, self.pool_size)\n",
        "        outputs = d_outputs.copy_to_host()\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        batch_size,num_channels, output_height, output_width = output_gradient.shape\n",
        "        d_input_grad = cuda.device_array(self.inputs.shape)\n",
        "        d_output_grad = cuda.to_device(output_gradient)\n",
        "        block_size = (8, 4, 4)\n",
        "        grid_size = ((batch_size-1)//block_size[0]+1, (output_height-1) //\n",
        "                      block_size[1]+1, (output_width-1)//block_size[2]+1)\n",
        "        maxPool2D_backward_kernel[grid_size, block_size](\n",
        "            self.d_inputs, d_input_grad, d_output_grad, self.stride, self.pool_size)\n",
        "        input_gradient = d_input_grad.copy_to_host()\n",
        "        return input_gradient\n",
        "\n",
        "\n",
        "    def init_weight(self):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uATv-h9kp5M2"
      },
      "source": [
        "Design - MaxPooling\n",
        "\n",
        "The MaxPooling algorithm mechanism is similar to the CNN Convolution Layer\n",
        "So the design using smem,rmem is similar to the Convolution class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h54KqhhLN6Hh"
      },
      "source": [
        "* Forward Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIyYZuplN6Hh"
      },
      "outputs": [],
      "source": [
        "input_shape=(32,200,200)\n",
        "inputs = np.random.randint(0,255,(64,*input_shape))/255\n",
        "maxp = MaxPool2D(2,2,input_shape=input_shape)\n",
        "%time out_host=maxp.forward(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ih0lIT8N6Hh"
      },
      "source": [
        "* Backward Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhK8ULkhN6Hh"
      },
      "outputs": [],
      "source": [
        "%time in_grad_host=maxp.backward(out_host,0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwLqmAfeN6Hh"
      },
      "outputs": [],
      "source": [
        "# @title Dense Layer\n",
        "\n",
        "@cuda.jit\n",
        "def dense_forward_kernel(inputs, weights, bias, outputs):\n",
        "    row, col = cuda.grid(2)\n",
        "    height = weights.shape[0]\n",
        "    if(row >= outputs.shape[0] or col >= outputs.shape[1]):\n",
        "        return\n",
        "    sum = 0\n",
        "    for i in range(inputs.shape[1]):\n",
        "        sum += inputs[row, i] * weights[i, col]\n",
        "    outputs[row, col] = sum + bias[0, col]\n",
        "\n",
        "\n",
        "\n",
        "class Dense(Layer):\n",
        "    def __init__(self, num_outputs, activation=None, input_shape=100):\n",
        "        self.num_outputs = num_outputs\n",
        "        self.biases = np.zeros((1, num_outputs))\n",
        "        self.activation = activation\n",
        "        self.use_device = False\n",
        "        self.inputs = None\n",
        "        self.input_shape = input_shape\n",
        "        self.init_weight()\n",
        "\n",
        "    def init_weight(self):\n",
        "        self.weights = np.random.randn(\n",
        "            self.input_shape, self.num_outputs) / self.num_outputs\n",
        "\n",
        "    def get_out_shape(self):\n",
        "        return self.num_outputs\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        assert self.input_shape == inputs.shape[-1], \"Input shape incorrect\"\n",
        "        block_size = (8, 4)\n",
        "        grid_size = ((inputs.shape[0]-1)//block_size[0]+1,\n",
        "                     (self.num_outputs-1)//block_size[1]+1)\n",
        "        # if(grid_size[0]*grid_size[1] < 128):\n",
        "        #     self.use_device = False\n",
        "        # outputs = None\n",
        "        # if(self.use_device == False):\n",
        "        #     outputs = np.dot(inputs, self.weights) + self.biases\n",
        "        # else:\n",
        "        self.d_weights = cuda.to_device(self.weights)\n",
        "        self.d_biases = cuda.to_device(self.biases)\n",
        "        d_outputs = cuda.device_array((inputs.shape[0], self.num_outputs))\n",
        "        self.d_inputs = cuda.to_device(inputs)\n",
        "        start = time.time()\n",
        "        dense_forward_kernel[grid_size, block_size](\n",
        "            self.d_inputs, self.d_weights, self.d_biases, d_outputs)\n",
        "        outputs = d_outputs.copy_to_host()\n",
        "\n",
        "        # if(self.activation==\"relu\"):\n",
        "        #   outputs = np.maximum(0,outputs)\n",
        "        if self.activation == \"softmax\":\n",
        "            outputs = self.softmax(outputs)\n",
        "        return outputs\n",
        "\n",
        "    def softmax(self, x):\n",
        "        e_x = np.exp(x-np.max(x, axis=1, keepdims=True))\n",
        "        return e_x/e_x.sum(axis=1, keepdims=True)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        # start = time.time()\n",
        "        # input_grad=None\n",
        "        # if(self.use_device==False):\n",
        "\n",
        "        input_grad = np.dot(output_gradient, self.weights.T)\n",
        "        weights_gradient = np.dot(self.inputs.T, output_gradient)\n",
        "        biases_gradient = np.sum(output_gradient, axis=0, keepdims=True)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights -= learning_rate * weights_gradient\n",
        "        self.biases -= learning_rate * biases_gradient\n",
        "\n",
        "        return input_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcRu0NBTN6Hi"
      },
      "outputs": [],
      "source": [
        "inputs=np.random.randint(1,255, (256,10000))/255\n",
        "dense=Dense(1024, input_shape= 10000)\n",
        "%time out_host=dense.forward(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu_-ftS0N6Hi"
      },
      "source": [
        "## Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTxnIbXNN6Hi"
      },
      "outputs": [],
      "source": [
        "modelIV=CNNModel([\n",
        "    Convolution(n_filters=16, filter_size=3, stride=1,activation='relu',input_shape=(1,28,28)),\n",
        "    MaxPool2D(pool_size=2),\n",
        "    Convolution(n_filters=32, filter_size=3, stride=1,activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(128),\n",
        "    Dense(10, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WbhkSTrN6Hi"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "modelIV.fit(x_train,y_train, epochs=3, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8XtPoANv47E"
      },
      "outputs": [],
      "source": [
        "\n",
        "%time y_predict =modelIV.predict(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYqhW7eQpfOg"
      },
      "source": [
        "# V. EVALUATE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4spivXZ9k2r"
      },
      "source": [
        "Evaluation script\n",
        "- To test the CNN model running on host and device, we use the Mnist data set including 60,000 training sets and 10,000 test sets.\n",
        "- Training on 3epoch and batch size is 128.\n",
        "- The model is built as follows:\n",
        "    Convolution(n_filters=16, filter_size=3, stride=1,activation='relu',input_shape=(1,28,28)),\n",
        "    MaxPool2D(pool_size=2),\n",
        "    Convolution(n_filters=32, filter_size=3, stride=1, activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(128),\n",
        "    Dense(10, activation='softmax')\n",
        "- Training results show that the model's accuracy is relatively good (>95% after 3 epochs)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do3TZThgW2PC"
      },
      "source": [
        "Result\n",
        "\n",
        "Runtime comparison table between sequential and parallel installation versions\n",
        "\n",
        "|           |CPU V1 | CPU V2 |  GPU V1 | GPU V2  |\n",
        "|-----------|:-----:|:-----: |:-------:|:-------:|\n",
        "|Convolution|3m46   | 11.7 s |   974ms |  472ms  |         \n",
        "|MaxPooling |19s    | 2.5s   |   551ms |  517ms  |         \n",
        "|Dense      |292ms  | 292ms  |  289ms  |  289ms  |         \n",
        "|Training   | inf   |  166ms |   2m2   |  1m36   |        \n",
        "|Testing    |17m34  |1m32    | 1.46s   |  1.28s  |   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPrs9obvdOH2"
      },
      "source": [
        "\n",
        "Comment\n",
        "```\n",
        "-In the sequential version installed on python, the calculation speed is very slow, especially the backward calculation step (sequential v1)\n",
        "- The speed of serialization v1 is improved when using the numpy calculation library (sequential v2)\n",
        "- After installing the parallel version on cuda the results are much better than the previous self version (parallel v1)\n",
        "- Effective use of memory such as SMEM, RMEM contributes to shortening optimal memory access time (parallel v2)\n",
        "- In general, among the three real-time layers, Convolution is the largest, followed by Maxpooling and finally Dense (taking up almost negligible time)\n",
        "=> For the CNN image recognition problem, using GPUs for parallel processing is extremely necessary and especially important to make the training and research process faster and easier.\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}